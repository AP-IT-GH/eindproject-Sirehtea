behaviors:
  Verstoppertje:
    trainer_type: ppo # Using PPO algorithm for training
    hyperparameters:
      batch_size: 128 # Number of experiences used for each update step
      buffer_size: 2048 # Number of experiences collected before updating the model
      learning_rate: 0.0003 # Initial learning rate for training
      beta: 0.01 # Entropy regularization strength
      epsilon: 0.2 # PPO clip parameter
      lambd: 0.95 # GAE parameter
      num_epoch: 3 # Number of passes over the training data per update
      learning_rate_schedule: linear # Linearly decrease learning rate over time
    network_settings:
      normalize: false # Do not normalize input features
      hidden_units: 512 # Number of hidden units in the neural network
      num_layers: 2 # Number of hidden layers
      vis_encode_type: simple # Simple encoder for visual input
    reward_signals:
      extrinsic:
        gamma: 0.99 # Discount factor for future rewards
        strength: 1.0 # Strength of the extrinsic reward signal
      curiosity:
        gamma: 0.99 # Discount factor for future curiosity rewards
        strength: 0.02 # Strength of the curiosity reward signal
        network_settings:
          hidden_units: 256 # Number of hidden units in the curiosity network
        learning_rate: 0.0003 # Learning rate for the curiosity module
    keep_checkpoints: 5 # Number of checkpoints to keep
    max_steps: 10000000 # Maximum number of training steps
    time_horizon: 128 # Steps over which experience is accumulated
    summary_freq: 30000 # Frequency of writing training statistics to summary
