{
    "name": "root",
    "gauges": {
        "AgentController.Policy.Entropy.mean": {
            "value": 1.3112913370132446,
            "min": 1.3112913370132446,
            "max": 1.4176894426345825,
            "count": 18
        },
        "AgentController.Policy.Entropy.sum": {
            "value": 65413.765625,
            "min": 65413.765625,
            "max": 73372.515625,
            "count": 18
        },
        "AgentController.Environment.EpisodeLength.mean": {
            "value": 13.725184094256258,
            "min": 13.725184094256258,
            "max": 148.30346820809248,
            "count": 18
        },
        "AgentController.Environment.EpisodeLength.sum": {
            "value": 46597.0,
            "min": 44008.0,
            "max": 51787.0,
            "count": 18
        },
        "AgentController.Step.mean": {
            "value": 899997.0,
            "min": 49970.0,
            "max": 899997.0,
            "count": 18
        },
        "AgentController.Step.sum": {
            "value": 899997.0,
            "min": 49970.0,
            "max": 899997.0,
            "count": 18
        },
        "AgentController.Policy.ExtrinsicValueEstimate.mean": {
            "value": -3.072711229324341,
            "min": -3.290557861328125,
            "max": -0.06088004633784294,
            "count": 18
        },
        "AgentController.Policy.ExtrinsicValueEstimate.sum": {
            "value": -10434.927734375,
            "min": -11145.119140625,
            "max": -60.210365295410156,
            "count": 18
        },
        "AgentController.Environment.CumulativeReward.mean": {
            "value": -4.593050647820966,
            "min": -6.2566653402308,
            "max": -2.439201451905626,
            "count": 18
        },
        "AgentController.Environment.CumulativeReward.sum": {
            "value": -15598.0,
            "min": -18462.0,
            "max": -950.0,
            "count": 18
        },
        "AgentController.Policy.ExtrinsicReward.mean": {
            "value": -4.593050647820966,
            "min": -6.2566653402308,
            "max": -2.439201451905626,
            "count": 18
        },
        "AgentController.Policy.ExtrinsicReward.sum": {
            "value": -15598.0,
            "min": -18462.0,
            "max": -950.0,
            "count": 18
        },
        "AgentController.Losses.PolicyLoss.mean": {
            "value": 0.024608819077257066,
            "min": 0.020164253004065054,
            "max": 0.026506960728147535,
            "count": 18
        },
        "AgentController.Losses.PolicyLoss.sum": {
            "value": 0.12304409538628533,
            "min": 0.08859617380827937,
            "max": 0.127661398689573,
            "count": 18
        },
        "AgentController.Losses.ValueLoss.mean": {
            "value": 22.137991027832033,
            "min": 2.2743107413252197,
            "max": 22.873311551411945,
            "count": 18
        },
        "AgentController.Losses.ValueLoss.sum": {
            "value": 110.68995513916016,
            "min": 9.097242965300879,
            "max": 111.9246400197347,
            "count": 18
        },
        "AgentController.Policy.LearningRate.mean": {
            "value": 0.00027382944272352197,
            "min": 0.00027382944272352197,
            "max": 0.00029922840775719754,
            "count": 18
        },
        "AgentController.Policy.LearningRate.sum": {
            "value": 0.0013691472136176097,
            "min": 0.00110701086099639,
            "max": 0.0014892080435973197,
            "count": 18
        },
        "AgentController.Policy.Epsilon.mean": {
            "value": 0.19127647799999997,
            "min": 0.19127647799999997,
            "max": 0.19974280249999998,
            "count": 18
        },
        "AgentController.Policy.Epsilon.sum": {
            "value": 0.9563823899999999,
            "min": 0.76900361,
            "max": 0.9964026800000002,
            "count": 18
        },
        "AgentController.Policy.Beta.mean": {
            "value": 0.0045646962522,
            "min": 0.0045646962522,
            "max": 0.004987165844750001,
            "count": 18
        },
        "AgentController.Policy.Beta.sum": {
            "value": 0.022823481261,
            "min": 0.018453280139,
            "max": 0.024820493732,
            "count": 18
        },
        "AgentController.IsTraining.mean": {
            "value": 1.0,
            "min": 1.0,
            "max": 1.0,
            "count": 18
        },
        "AgentController.IsTraining.sum": {
            "value": 1.0,
            "min": 1.0,
            "max": 1.0,
            "count": 18
        },
        "HunterController.Policy.Entropy.mean": {
            "value": 1.2172000408172607,
            "min": 1.2172000408172607,
            "max": 1.418807029724121,
            "count": 18
        },
        "HunterController.Policy.Entropy.sum": {
            "value": 60720.0234375,
            "min": 60720.0234375,
            "max": 73430.359375,
            "count": 18
        },
        "HunterController.Environment.EpisodeLength.mean": {
            "value": 13.725184094256258,
            "min": 13.725184094256258,
            "max": 148.30346820809248,
            "count": 18
        },
        "HunterController.Environment.EpisodeLength.sum": {
            "value": 46597.0,
            "min": 44008.0,
            "max": 51787.0,
            "count": 18
        },
        "HunterController.Step.mean": {
            "value": 899997.0,
            "min": 49970.0,
            "max": 899997.0,
            "count": 18
        },
        "HunterController.Step.sum": {
            "value": 899997.0,
            "min": 49970.0,
            "max": 899997.0,
            "count": 18
        },
        "HunterController.Policy.ExtrinsicValueEstimate.mean": {
            "value": 4.860969066619873,
            "min": -0.30832013487815857,
            "max": 4.909704685211182,
            "count": 18
        },
        "HunterController.Policy.ExtrinsicValueEstimate.sum": {
            "value": 16507.8515625,
            "min": -304.9286193847656,
            "max": 16595.3125,
            "count": 18
        },
        "HunterController.Environment.CumulativeReward.mean": {
            "value": 9.246171967020024,
            "min": -7.7727272727272725,
            "max": 9.643356643356643,
            "count": 18
        },
        "HunterController.Environment.CumulativeReward.sum": {
            "value": 31400.0,
            "min": -2565.0,
            "max": 31585.0,
            "count": 18
        },
        "HunterController.Policy.ExtrinsicReward.mean": {
            "value": 9.246171967020024,
            "min": -7.7727272727272725,
            "max": 9.643356643356643,
            "count": 18
        },
        "HunterController.Policy.ExtrinsicReward.sum": {
            "value": 31400.0,
            "min": -2565.0,
            "max": 31585.0,
            "count": 18
        },
        "HunterController.Losses.PolicyLoss.mean": {
            "value": 0.021755878884772147,
            "min": 0.021755878884772147,
            "max": 0.026783958085773822,
            "count": 18
        },
        "HunterController.Losses.PolicyLoss.sum": {
            "value": 0.10877939442386074,
            "min": 0.0880014535949158,
            "max": 0.13391979042886912,
            "count": 18
        },
        "HunterController.Losses.ValueLoss.mean": {
            "value": 3.532894474665324,
            "min": 1.2711303182442983,
            "max": 3.532894474665324,
            "count": 18
        },
        "HunterController.Losses.ValueLoss.sum": {
            "value": 17.66447237332662,
            "min": 6.355651591221491,
            "max": 17.66447237332662,
            "count": 18
        },
        "HunterController.Policy.LearningRate.mean": {
            "value": 0.00027382944272352197,
            "min": 0.00027382944272352197,
            "max": 0.00029922840775719754,
            "count": 18
        },
        "HunterController.Policy.LearningRate.sum": {
            "value": 0.0013691472136176097,
            "min": 0.00110701086099639,
            "max": 0.0014892080435973197,
            "count": 18
        },
        "HunterController.Policy.Epsilon.mean": {
            "value": 0.19127647799999997,
            "min": 0.19127647799999997,
            "max": 0.19974280249999998,
            "count": 18
        },
        "HunterController.Policy.Epsilon.sum": {
            "value": 0.9563823899999999,
            "min": 0.76900361,
            "max": 0.9964026800000002,
            "count": 18
        },
        "HunterController.Policy.Beta.mean": {
            "value": 0.0045646962522,
            "min": 0.0045646962522,
            "max": 0.004987165844750001,
            "count": 18
        },
        "HunterController.Policy.Beta.sum": {
            "value": 0.022823481261,
            "min": 0.018453280139,
            "max": 0.024820493732,
            "count": 18
        },
        "HunterController.IsTraining.mean": {
            "value": 1.0,
            "min": 1.0,
            "max": 1.0,
            "count": 18
        },
        "HunterController.IsTraining.sum": {
            "value": 1.0,
            "min": 1.0,
            "max": 1.0,
            "count": 18
        }
    },
    "metadata": {
        "timer_format_version": "0.1.0",
        "start_time_seconds": "1717871129",
        "python_version": "3.9.18 (main, Sep 11 2023, 14:09:26) [MSC v.1916 64 bit (AMD64)]",
        "command_line_arguments": "C:\\Users\\turga\\anaconda3\\envs\\VR_Experience\\Scripts\\mlagents-learn config/multitraining.yaml --run-id=HunterRun --force",
        "mlagents_version": "0.30.0",
        "mlagents_envs_version": "0.30.0",
        "communication_protocol_version": "1.5.0",
        "pytorch_version": "1.7.1+cu110",
        "numpy_version": "1.21.2",
        "end_time_seconds": "1717872770"
    },
    "total": 1640.6842319,
    "count": 1,
    "self": 0.01037520000022596,
    "children": {
        "run_training.setup": {
            "total": 0.10218349999999998,
            "count": 1,
            "self": 0.10218349999999998
        },
        "TrainerController.start_learning": {
            "total": 1640.5716731999999,
            "count": 1,
            "self": 1.347589000000653,
            "children": {
                "TrainerController._reset_env": {
                    "total": 10.663283,
                    "count": 1,
                    "self": 10.663283
                },
                "TrainerController.advance": {
                    "total": 1628.3865304999993,
                    "count": 44616,
                    "self": 1.6936126999992211,
                    "children": {
                        "env_step": {
                            "total": 888.3329736000022,
                            "count": 44616,
                            "self": 752.8428846000144,
                            "children": {
                                "SubprocessEnvManager._take_step": {
                                    "total": 134.80265480000486,
                                    "count": 44616,
                                    "self": 3.3340558000016927,
                                    "children": {
                                        "TorchPolicy.evaluate": {
                                            "total": 131.46859900000317,
                                            "count": 33982,
                                            "self": 131.46859900000317
                                        }
                                    }
                                },
                                "workers": {
                                    "total": 0.6874341999829703,
                                    "count": 44615,
                                    "self": 0.0,
                                    "children": {
                                        "worker_root": {
                                            "total": 1568.7346853000151,
                                            "count": 44615,
                                            "is_parallel": true,
                                            "self": 994.1396497000183,
                                            "children": {
                                                "steps_from_proto": {
                                                    "total": 0.006458600000000203,
                                                    "count": 2,
                                                    "is_parallel": true,
                                                    "self": 0.0016153000000027617,
                                                    "children": {
                                                        "_process_rank_one_or_two_observation": {
                                                            "total": 0.004843299999997441,
                                                            "count": 8,
                                                            "is_parallel": true,
                                                            "self": 0.004843299999997441
                                                        }
                                                    }
                                                },
                                                "UnityEnvironment.step": {
                                                    "total": 574.5885769999969,
                                                    "count": 44615,
                                                    "is_parallel": true,
                                                    "self": 23.658934299962084,
                                                    "children": {
                                                        "UnityEnvironment._generate_step_input": {
                                                            "total": 33.2011405000029,
                                                            "count": 44615,
                                                            "is_parallel": true,
                                                            "self": 33.2011405000029
                                                        },
                                                        "communicator.exchange": {
                                                            "total": 461.6331020000116,
                                                            "count": 44615,
                                                            "is_parallel": true,
                                                            "self": 461.6331020000116
                                                        },
                                                        "steps_from_proto": {
                                                            "total": 56.09540020002037,
                                                            "count": 89230,
                                                            "is_parallel": true,
                                                            "self": 14.373319100020097,
                                                            "children": {
                                                                "_process_rank_one_or_two_observation": {
                                                                    "total": 41.722081100000274,
                                                                    "count": 356920,
                                                                    "is_parallel": true,
                                                                    "self": 41.722081100000274
                                                                }
                                                            }
                                                        }
                                                    }
                                                }
                                            }
                                        }
                                    }
                                }
                            }
                        },
                        "trainer_advance": {
                            "total": 738.3599441999977,
                            "count": 89230,
                            "self": 3.889134399992713,
                            "children": {
                                "process_trajectory": {
                                    "total": 371.2460398000055,
                                    "count": 89230,
                                    "self": 371.0427551000055,
                                    "children": {
                                        "RLTrainer._checkpoint": {
                                            "total": 0.20328470000004017,
                                            "count": 2,
                                            "self": 0.20328470000004017
                                        }
                                    }
                                },
                                "_update_policy": {
                                    "total": 363.22476999999947,
                                    "count": 182,
                                    "self": 262.2585266000053,
                                    "children": {
                                        "TorchPPOOptimizer.update": {
                                            "total": 100.96624339999411,
                                            "count": 5460,
                                            "self": 100.96624339999411
                                        }
                                    }
                                }
                            }
                        }
                    }
                },
                "TrainerController._save_models": {
                    "total": 0.17427069999985179,
                    "count": 1,
                    "self": 0.026336700000001656,
                    "children": {
                        "RLTrainer._checkpoint": {
                            "total": 0.14793399999985013,
                            "count": 2,
                            "self": 0.14793399999985013
                        }
                    }
                }
            }
        }
    }
}